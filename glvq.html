

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Generalized Learning Vector Quantization &mdash; sklearn-glvq 1.1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Robust Soft Learning Vector Quantization" href="rslvq.html" />
    <link rel="prev" title="GLVQ Benchmark" href="auto_examples/plot_benchmark.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> sklearn-glvq
          

          
          </a>

          
            
            
              <div class="version">
                1.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="modules/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Examples</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Generalized Learning Vector Quantization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#generalized-relevance-learning-vector-quantization-grlvq">Generalized Relevance Learning Vector Quantization (GRLVQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generalized-matrix-learning-vector-quantization-gmlvq">Generalized Matrix Learning Vector Quantization (GMLVQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#localized-generalized-matrix-learning-vector-quantization-lgmlvq">Localized Generalized Matrix Learning Vector Quantization (LGMLVQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-details">Implementation Details</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rslvq.html">Robust Soft Learning Vector Quantization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">sklearn-glvq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Generalized Learning Vector Quantization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/glvq.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="generalized-learning-vector-quantization">
<span id="glvq"></span><h1>Generalized Learning Vector Quantization<a class="headerlink" href="#generalized-learning-vector-quantization" title="Permalink to this headline">¶</a></h1>
<p>A GLVQ model can be constructed by initializing <a class="reference internal" href="modules/generated/sklearn_lvq.GlvqModel.html#sklearn_lvq.GlvqModel" title="sklearn_lvq.GlvqModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GlvqModel</span></code></a> with the
desired hyper-parameters, e.g. the number of prototypes, and the initial
positions of the prototypes and then calling the <a class="reference internal" href="modules/generated/sklearn_lvq.GlvqModel.html#sklearn_lvq.GlvqModel.fit" title="sklearn_lvq.GlvqModel.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">GlvqModel.fit</span></code></a> function with the
input data. The resulting model will contain the learned prototype
positions and prototype labels which can be retrieved as properties <cite>w_</cite>
and <cite>c_w_</cite>. Classifications of new data can be made via the predict
function, which computes the Euclidean distances of the input data to
all prototypes and returns the label of the respective closest prototypes.</p>
<p>Placing the prototypes is done by optimizing the following cost
function, called the Generalized Learning Vector Quantization (GLVQ)
cost function <a class="footnote-reference" href="#id2" id="id1">[2]</a>:</p>
<p><img class="math" src="_images/math/0e428cb1bef91f169fcf18f5515298467718903c.png" alt="\displaystyle E_{\mathrm{GLVQ}} = \sum_{i=1}^l \Phi\left(\frac{d^+_i - d^-_i}{d^+_i + d^-_i}\right)"/></p>
<p>where <img class="math" src="_images/math/a16d1f7a4e063b183ad6861103bc3a7681db7340.png" alt="d^+_i"/> is the squared Euclidean distance of <img class="math" src="_images/math/7720e563212e11bf72de255ab82c2a3b97c1a7f5.png" alt="x_i"/> to the closest
prototype <img class="math" src="_images/math/a5ed050c0bfcf35b4d60d048b02fa167cc3667a7.png" alt="w_k"/> with the same label <img class="math" src="_images/math/da669269e0074b69cb05673636798e853c2026e2.png" alt="(y_i = c_k)"/> and <img class="math" src="_images/math/bd64c85b1e9b99b95c7bbcbf620f3eb6704320e2.png" alt="d^-_i"/> is the squared
Euclidean distance of <img class="math" src="_images/math/7720e563212e11bf72de255ab82c2a3b97c1a7f5.png" alt="x_i"/> to the closest prototype w_k with a different
label <img class="math" src="_images/math/b77bced86f6546b404576b7e9f1e10948b090fa8.png" alt="(y_i \neq c_k)"/>. Note that an LVQ model will classify a data point
correctly if and only if <img class="math" src="_images/math/ba907fa70620472b065fe58c843881554aac93df.png" alt="d^+_i &lt; d^-_i"/>, which makes the cost function a
coarse approximation of the classification error.</p>
<p>The optimization is performed via a limited-memory version of the
Broyden-Fletcher-Goldfarb-Shanno algorithm. Regarding runtime, the cost
function can be computed in linear time with respect to the data points:
For each data point, we need to compute the distances to all prototypes,
compute the fraction <img class="math" src="_images/math/0a3acf24011cee0e8e58fa621ddb17c9efaae3dd.png" alt="(d^+_i - d^-_i) / (d^+_i + d^-_i)"/> and then sum up all
these fractions, the same goes for the derivative. Thus, GLVQ scales
linearly with the number of data points.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[2]</a></td><td><a class="reference external" href="https://papers.nips.cc/paper/1113-generalized-learning-vector-quantization.pdf">“Generalized learning vector quantization.”</a>
Sato, Atsushi, and Keiji Yamada - Advances in neural information processing systems 8, pp. 423-429, 1996.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="generalized-relevance-learning-vector-quantization-grlvq">
<h2>Generalized Relevance Learning Vector Quantization (GRLVQ)<a class="headerlink" href="#generalized-relevance-learning-vector-quantization-grlvq" title="Permalink to this headline">¶</a></h2>
<p>In most classification tasks, some features are more discriminative than
others. Generalized Relevance Learning Vector Quantization (GRLVQ)
accounts for that by weighting each feature j with a relevance weight
<img class="math" src="_images/math/ab09ea3d9c6a9eee32d16f4d0fde9af8f165379e.png" alt="\lambda_j"/>, such that all relevances are <img class="math" src="_images/math/f37e6efc0713a22751c035108f69071fd8e8cf17.png" alt="\geq 0"/> and sum up to 1. The
relevances are optimized using LBFGS on the same cost function mentioned
above, just with respect to the relevance terms.
Beyond enhanced classification accuracy, the relevance weights obtained
by GRLVQ can also be used to obtain a dimensionality reduction by
throwing out features with low (or zero) relevance. After initializing a
GrlvqModel and calling the fit function with your data set, you can
retrieve the learned relevances via the attribute <cite>lambda_</cite>.</p>
<p>The following figure shows how GRLVQ classifies some example data after
training. The blue dots show represent the prototype. The yellow and
purple dots are the data points. The bigger transparent circle represent
the target value and the smaller circle the predicted target value. The
right side plot shows the data and prototypes multiplied with the
feature relevances. As can be seen, GRLVQ correctly dismisses the second
dimension, which is non-discriminative, and emphasizes the first
dimension, which is sufficient for class discrimination.</p>
<a class="reference external image-reference" href="auto_examples/plot_grlvq.html"><img alt="_images/sphx_glr_plot_grlvq_001.png" src="_images/sphx_glr_plot_grlvq_001.png" /></a>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li>“Generalized relevance learning vector quantization”
B. Hammer and T. Villmann - Neural Networks, 15, 1059-1068, 2002.</li>
</ul>
</div>
</div>
<div class="section" id="generalized-matrix-learning-vector-quantization-gmlvq">
<h2>Generalized Matrix Learning Vector Quantization (GMLVQ)<a class="headerlink" href="#generalized-matrix-learning-vector-quantization-gmlvq" title="Permalink to this headline">¶</a></h2>
<p>Generalized Matrix Learning Vector Quantization (GMLVQ) generalizes over
GRLVQ by not only weighting features but learning a full linear
transformation matrix <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> to support classification. Equivalently,
this matrix <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> can be seen as a distortion of the Euclidean distance
in order to make data points from the same class look more similar and
data points from different classes look more dissimilar, via the
following equation:</p>
<div class="math">
<p><img src="_images/math/fca7cb685514a95d3ff0b27d0a818c2aae8919b5.png" alt="||\Omega \cdot x_i - \Omega \cdot w_k||^2 = (\Omega \cdot x_i - \Omega
\cdot w_k)^T \cdot (\Omega \cdot x_i - \Omega \cdot w_k) = (x_i - w_k)^T
\cdot \Omega^T \cdot \Omega \cdot (x_i - w_k)"/></p>
</div><p>The matrix product <img class="math" src="_images/math/c262fd250c1539669018ad8aeedc443c75fa13de.png" alt="\Omega^T \cdot \Omega"/> is also called the positive
semi-definite <em>relevance matrix</em> <img class="math" src="_images/math/5b99ed91598429157b547a2c3700a6b25c52824a.png" alt="\Lambda"/>. Interpreted this way, GMLVQ is
a <em>metric learning</em> algorithm <a class="footnote-reference" href="#id6" id="id4">[3]</a>. It is also possible to initialize the
GmlvqModel by setting the dim parameter to an integer less than the data
dimensionality, in which case Omega will have only dim rows, performing
an implicit dimensionality reduction. This variant is called Limited
Rank Matrix LVQ or LiRaM-LVQ <a class="footnote-reference" href="#id7" id="id5">[4]</a>. After initializing the GmlvqModel and
calling the fit function on your data set, the learned <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> matrix can
be retrieved via the attribute <cite>omega_</cite>.</p>
<p>The following figure shows how GMLVQ classifies some example data after
training. The blue dots show represent the prototype. The yellow and
purple dots are the data points. The bigger transparent circle represent
the target value and the smaller circle the predicted target value. The
right side plot shows the data and prototypes multiplied with the
learned <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> matrix. As can be seen, GMLVQ effectively projects the
data onto a one-dimensinal line such that both classes are well
distinguished. Note that this projection would not have been possible
for GRLVQ because the relevant data direction is not parallel to a
coordinate axis.</p>
<a class="reference external image-reference" href="auto_examples/plot_gmlvq.html"><img alt="_images/sphx_glr_plot_gmlvq_001.png" src="_images/sphx_glr_plot_gmlvq_001.png" /></a>
<div class="topic">
<p class="topic-title first">References:</p>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[3]</a></td><td><a class="reference external" href="http://www.cs.rug.nl/~biehl/Preprints/gmlvq.pdf">“Adaptive Relevance Matrices in Learning Vector Quantization”</a>
Petra Schneider, Michael Biehl and Barbara Hammer - Neural Computation, vol. 21, nb. 12, pp. 3532-3561, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[4]</a></td><td><a class="reference external" href="http://www.cs.rug.nl/biehl/Preprints/liram-preliminary.pdf">“Limited Rank Matrix Learning - Discriminative Dimension Reduction and Visualization”</a>
K. Bunte, P. Schneider, B. Hammer, F.-M. Schleif, T. Villmann and M. Biehl - Neural Networks, vol. 26, nb. 4, pp. 159-173, 2012.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="localized-generalized-matrix-learning-vector-quantization-lgmlvq">
<h2>Localized Generalized Matrix Learning Vector Quantization (LGMLVQ)<a class="headerlink" href="#localized-generalized-matrix-learning-vector-quantization-lgmlvq" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="modules/generated/sklearn_lvq.LgmlvqModel.html#sklearn_lvq.LgmlvqModel" title="sklearn_lvq.LgmlvqModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LgmlvqModel</span></code></a> extends GLVQ by giving each prototype/class relevances for each feature. This way LGMLVQ is able to project the data for better
classification.</p>
<p>Especially in multi-class data sets, the ideal projection <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> may be
different for each class, or even each prototype. Localized Generalized
Matrix Learning Vector Quantization (LGMLVQ) accounts for this locality
dependence by learning an individual <img class="math" src="_images/math/a0c92b2d911ecaed5d7d15046033c5fe527e0878.png" alt="\Omega_k"/> for each prototype k <a class="footnote-reference" href="#id9" id="id8">[5]</a>.
As with GMLVQ, the rank of <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> can be bounded by using the dim
parameter. After initializing the <a class="reference internal" href="modules/generated/sklearn_lvq.LgmlvqModel.html#sklearn_lvq.LgmlvqModel" title="sklearn_lvq.LgmlvqModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LgmlvqModel</span></code></a> and calling the fit
function on your data set, the learned <img class="math" src="_images/math/a0c92b2d911ecaed5d7d15046033c5fe527e0878.png" alt="\Omega_k"/> matrices can be
retrieved via the attribute <cite>omegas_</cite>.</p>
<p>The following figure shows how LGMLVQ classifies some example data after
training. The blue dots show represent the prototype. The yellow and
purple dots are the data points. The bigger transparent circle represent
the target value and the smaller circle the predicted target value. The
plot in the middle and on the right show the data and prototypes after
multiplication with the <img class="math" src="_images/math/34b719c3420fe5c82d20557db6d38dc66814f261.png" alt="\Omega_1"/> and <img class="math" src="_images/math/cda0935ebe22bbaf72367c32323890d7de5ebf08.png" alt="\Omega_2"/> matrix respectively. As
can be seen, both prototypes project the data onto one dimension, but
they choose orthogonal projection dimensions, such that the data of the
respective own class is close while the other class gets dispersed,
thereby enhancing classification accuracy. A <a class="reference internal" href="modules/generated/sklearn_lvq.GmlvqModel.html#sklearn_lvq.GmlvqModel" title="sklearn_lvq.GmlvqModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GmlvqModel</span></code></a> can not solve
this classification problem, because no global <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> can enhance the
classification significantly.</p>
<a class="reference external image-reference" href="auto_examples/plot_lgmlvq.html"><img alt="_images/sphx_glr_plot_lgmlvq_001.png" src="_images/sphx_glr_plot_lgmlvq_001.png" /></a>
<div class="topic">
<p class="topic-title first">References:</p>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[5]</a></td><td><a class="reference external" href="http://www.cs.rug.nl/~biehl/Preprints/gmlvq.pdf">“Adaptive Relevance Matrices in Learning Vector Quantization”</a>
Petra Schneider, Michael Biehl and Barbara Hammer - Neural Computation, vol. 21, nb. 12, pp. 3532-3561, 2009.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="implementation-details">
<h2>Implementation Details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<p>This implementation is based upon the reference Matlab implementation
provided by Biehl, Schneider and Bunte <a class="footnote-reference" href="#id13" id="id11">[6]</a>.</p>
<p>To optimize the GLVQ cost function with respect to all parameters
(prototype positions as well as relevances and <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"/> matrices) we
employ the LBFGS implementation of <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb">scipy</a>. To prevent the degeneration of relevances in GMLVQ, we add
the log-determinant of the relevance matrix <img class="math" src="_images/math/b8ae8f3cea8e8ea5f76fd98942f6298d07a32378.png" alt="\Lambda = \Omega^T \cdot \Omega"/>
to the cost function, such that relevances can not degenerate to
zero <a class="footnote-reference" href="#id14" id="id12">[7]</a>.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id11">[6]</a></td><td><a class="reference external" href="http://matlabserver.cs.rug.nl/gmlvqweb/web/">LVQ Toolbox</a>  M. Biehl, P. Schneider and K. Bunte, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id12">[7]</a></td><td><a class="reference external" href="http://www.cs.rug.nl/~biehl/Preprints/tnn-2010.pdf">“Regularization in Matrix Relevance Learning”</a>
P. Schneider, K. Bunte, B. Hammer and M. Biehl - IEEE Transactions on Neural Networks, vol. 21, nb. 5, pp. 831-840, 2010.</td></tr>
</tbody>
</table>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rslvq.html" class="btn btn-neutral float-right" title="Robust Soft Learning Vector Quantization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="auto_examples/plot_benchmark.html" class="btn btn-neutral" title="GLVQ Benchmark" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Joris Jensen

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>